{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended contennt.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are imported for you. If you prefer to use additional libraries feel free to uncomment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:56:30.025847Z",
     "start_time": "2020-02-11T21:56:28.449847Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "# from pprint import pprint\n",
    "# from lxml import html\n",
    "# from lxml.html import fromstring\n",
    "# import urllib.request\n",
    "# from urllib.request import urlopen\n",
    "import random\n",
    "# import scrapy\n",
    "# import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:56:33.812057Z",
     "start_time": "2020-02-11T21:56:32.201892Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:56:36.993188Z",
     "start_time": "2020-02-11T21:56:36.947460Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "list_articles = soup.find_all(\"article\", class_=\"Box-row d-flex\")\n",
    "list_developers =[]\n",
    "for article in list_articles:\n",
    "    developers = article.find_all(\"a\")\n",
    "    list_developers.append([x.text.strip().split(\"\\n\")[0] for x in developers])\n",
    "list_developers = [list(filter(None,x)) for x in list_developers]\n",
    "list_developers_out = [x[2]+\" (\"+x[1]+\")\" for x in list_developers]\n",
    "# print(list_developers_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:56:49.578295Z",
     "start_time": "2020-02-11T21:56:48.078273Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:56:51.261514Z",
     "start_time": "2020-02-11T21:56:51.200941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brettkromkamp / contextualise (Contextualise is a simple and flexible tool particularly suited for organising information-heavy projects and activities consisting of unstructured and widely diverse data and information resources)', 'eriklindernoren / ML-From-Scratch (Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.)', 'domlysz / BlenderGIS (Blender addons to make the bridge between Blender and geographic data)', 'cycz / jdBuyMask (京东监控口罩有货爬虫，自动下单爬虫，口罩爬虫)', 'xingyizhou / CenterNet (Object detection, 3D detection, and pose estimation using center point detection:)', 'tlbootcamp / tlroadmap (👩🏼\\u200d💻👨🏻\\u200d💻Карта навыков и модель развития тимлидов)', 'albumentations-team / albumentations (fast image augmentation library and easy to use wrapper around other libraries)', 'mingrammer / diagrams (🎨 Diagram as Code for prototyping cloud system architectures)', 'pypa / virtualenv (Virtual Python Environment builder)', 'Rlacat / jd-automask (防护-京东口罩自动抢购并下单)', 'kevinzakka / nca (A PyTorch implementation of Neighbourhood Components Analysis.)', 'robotframework / robotframework (Generic automation framework for acceptance testing and RPA)', 'python / mypy (Optional static typing for Python 3 and 2 (PEP 484))', 'openai / gym (A toolkit for developing and comparing reinforcement learning algorithms.)', 'explosion / spaCy (💫 Industrial-strength Natural Language Processing (NLP) with Python and Cython)', 'tensortrade-org / tensortrade (An open source reinforcement learning framework for training, evaluating, and deploying robust trading agents.)', 'fendouai / PyTorchDocs (PyTorch 官方中文教程包含 60 分钟快速入门教程，强化教程，计算机视觉，自然语言处理，生成对抗网络，强化学习。欢迎 Star，Fork！)', 'PrefectHQ / prefect (The Prefect Core workflow engine)', 'nodejs / node-gyp (Node.js native addon build tool)', 'satoshiiizuka / siggraphasia2019_remastering (Code for the paper \"DeepRemaster: Temporal Source-Reference Attention Networks for Comprehensive Video Enhancement\". http://iizuka.cs.tsukuba.ac.jp/projects/remastering/)', 'clovaai / CRAFT-pytorch (Official implementation of Character Region Awareness for Text Detection (CRAFT))', 'programthink / zhao (【编程随想】整理的《太子党关系网络》，专门揭露赵国的权贵)', 'tensorflow / models (Models and examples built with TensorFlow)', 'keras-team / keras (Deep Learning for humans)', 'cloud-custodian / cloud-custodian (Rules engine for cloud security, cost optimization, and governance, DSL in yaml for policies to query, filter, and take actions on resources)']\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "list_articles = soup.find_all(\"article\", class_=\"Box-row\")\n",
    "list_repos= [x.text.strip().split(\"\\n\") for x in list_articles]\n",
    "list_repos = [list(filter(None,map(str.strip, x))) for x in list_repos]\n",
    "list_repos_out = [x[1]+\" \"+x[2]+\" (\"+x[3]+\")\" for x in list_repos]\n",
    "print(list_repos_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:57:02.529409Z",
     "start_time": "2020-02-11T21:57:01.535844Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:57:03.513382Z",
     "start_time": "2020-02-11T21:57:03.494830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png', '//upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG', '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg', '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG', '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg', '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG', '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg', '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/4/44/The_Walt_Disney_Company_Logo.svg/120px-The_Walt_Disney_Company_Logo.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png', '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg', '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png', '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png', '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png', '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png', '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1', '/static/images/wikimedia-button.png', '/static/images/poweredby_mediawiki_88x31.png']\n"
     ]
    }
   ],
   "source": [
    "#your code\n",
    "list_articles = soup.find_all(\"img\")\n",
    "list_src = [x.get_attribute_list('src')[0] for x in list_articles]\n",
    "print(list_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:57:21.220770Z",
     "start_time": "2020-02-11T21:57:20.801506Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' \n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:57:37.718525Z",
     "start_time": "2020-02-11T21:57:36.203252Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code\n",
    "list_articles = soup.body.find(id=\"bodyContent\")\n",
    "list_articles = list_articles.find_all(\"li\")\n",
    "list_python   = [python for python in list_articles if(str(python).upper().find(\"PYT\")!=-1)]\n",
    "list_python   = [python.a.get_attribute_list('href')[0] for python in list_python]\n",
    "random_python = random.choice(list_python) \n",
    "urlp = 'https://en.wikipedia.org'+ random_python\n",
    "htmlp= requests.get(urlp).content\n",
    "soupp= BeautifulSoup(htmlp,'lxml')\n",
    "# soupp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:57:42.351119Z",
     "start_time": "2020-02-11T21:57:42.334200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#mw-head', '#p-search', '/wiki/Greek_language', '/wiki/Ancient_Greek', '/wiki/Isocrates', '/wiki/Pella', '/wiki/Peace_of_Philocrates', '/wiki/Philip_II_of_Macedon', '/wiki/Athens', '/wiki/Hegesippus_(orator)', '#cite_note-1', '/wiki/Demosthenes', '/wiki/Python_of_Aenus', '/wiki/Cotys_I_(Odrysian)', '/wiki/Odrysia', '#cite_note-2', '/w/index.php?title=Python_of_Byzantium&action=edit&section=1', '#cite_ref-1', '/wiki/International_Standard_Book_Number', '/wiki/Special:BookSources/3-515-08396-0', '#cite_ref-2', '/wiki/International_Standard_Book_Number', '/wiki/Special:BookSources/3-515-08396-0', '/wiki/File:Homer_British_Museum.jpg', '/wiki/Wikipedia:Stub', 'https://en.wikipedia.org/w/index.php?title=Python_of_Byzantium&action=edit', '/wiki/Template:AncientGreece-writer-stub', '/wiki/Template_talk:AncientGreece-writer-stub', 'https://en.wikipedia.org/w/index.php?title=Template:AncientGreece-writer-stub&action=edit', 'https://en.wikipedia.org/w/index.php?title=Python_of_Byzantium&oldid=787625673', '/wiki/Help:Category', '/wiki/Category:Ancient_Greek_rhetoricians', '/wiki/Category:Ancient_Greek_statesmen', '/wiki/Category:Ancient_Byzantines', '/wiki/Category:Ancient_Greeks_in_Macedon', '/wiki/Category:4th-century_BC_Greek_people', '/wiki/Category:Courtiers_of_Philip_II_of_Macedon', '/wiki/Category:Ambassadors_of_Macedonia_(ancient_kingdom)', '/wiki/Category:Ancient_Greek_writer_stubs', '/wiki/Category:Articles_containing_Greek-language_text', '/wiki/Category:All_stub_articles']\n"
     ]
    }
   ],
   "source": [
    "list_python_a = soupp.body.find(id=\"bodyContent\")\n",
    "list_python_a = list_python_a.find_all(\"a\")\n",
    "list_python_a = [python.get_attribute_list('href')[0] for python in list_python_a]\n",
    "print(list_python_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:57:52.687131Z",
     "start_time": "2020-02-11T21:57:50.709317Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:57:53.828691Z",
     "start_time": "2020-02-11T21:57:53.808389Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 18 - Crimes and Criminal Procedure ٭',\n",
       " 'Title 20 - Education',\n",
       " 'Title 26 - Internal Revenue Code']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_titles = soup.find_all(\"div\",{'class':'usctitlechanged'})\n",
    "list_titles = [title.text.strip() for title in list_titles]\n",
    "list_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:58:03.613766Z",
     "start_time": "2020-02-11T21:58:03.363441Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:58:06.341101Z",
     "start_time": "2020-02-11T21:58:06.324582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YASER ABDEL SAID', 'JASON DEREK BROWN', 'ALEXIS FLORES', 'EUGENE PALMER', 'SANTIAGO VILLALBA MEDEROS', 'RAFAEL CARO-QUINTERO', 'ROBERT WILLIAM FISHER', 'BHADRESHKUMAR CHETANBHAI PATEL', 'ARNOLDO JIMENEZ', 'ALEJANDRO ROSALES CASTILLO']\n"
     ]
    }
   ],
   "source": [
    "#your code \n",
    "list_fbi = soup.find(id='query-results-0f737222c5054a81a120bce207b0446a')\n",
    "list_fbi = list_fbi.find_all(\"h3\")\n",
    "list_fbi = [bad.text.strip() for bad in list_fbi]\n",
    "print(list_fbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:58:17.941055Z",
     "start_time": "2020-02-11T21:58:16.451832Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:58:20.459057Z",
     "start_time": "2020-02-11T21:58:20.307079Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>21:36:02</td>\n",
       "      <td>0.14N</td>\n",
       "      <td>130.68E</td>\n",
       "      <td>PAPUA REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>21:31:13</td>\n",
       "      <td>17.85N</td>\n",
       "      <td>66.94W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>21:21:19</td>\n",
       "      <td>16.34N</td>\n",
       "      <td>98.29W</td>\n",
       "      <td>OAXACA, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>21:10:51</td>\n",
       "      <td>27.90S</td>\n",
       "      <td>69.17W</td>\n",
       "      <td>ATACAMA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>21:04:21</td>\n",
       "      <td>4.74N</td>\n",
       "      <td>83.49E</td>\n",
       "      <td>NORTH INDIAN OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>21:02:35</td>\n",
       "      <td>40.54N</td>\n",
       "      <td>20.88E</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:45:16</td>\n",
       "      <td>38.86N</td>\n",
       "      <td>17.43E</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:40:51</td>\n",
       "      <td>35.12N</td>\n",
       "      <td>27.91E</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:38:32</td>\n",
       "      <td>2.89S</td>\n",
       "      <td>129.85E</td>\n",
       "      <td>SERAM, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:28:46</td>\n",
       "      <td>38.45N</td>\n",
       "      <td>25.67E</td>\n",
       "      <td>AEGEAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:23:50</td>\n",
       "      <td>17.89N</td>\n",
       "      <td>66.85W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:15:44</td>\n",
       "      <td>38.84N</td>\n",
       "      <td>122.76W</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:15:34</td>\n",
       "      <td>20.87S</td>\n",
       "      <td>70.82W</td>\n",
       "      <td>OFFSHORE TARAPACA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:15:32</td>\n",
       "      <td>19.17N</td>\n",
       "      <td>155.47W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:06:05</td>\n",
       "      <td>38.86N</td>\n",
       "      <td>16.81E</td>\n",
       "      <td>SOUTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>20:02:39</td>\n",
       "      <td>34.91N</td>\n",
       "      <td>27.94E</td>\n",
       "      <td>EASTERN MEDITERRANEAN SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>19:55:58</td>\n",
       "      <td>2.95S</td>\n",
       "      <td>119.38E</td>\n",
       "      <td>SULAWESI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>19:52:36</td>\n",
       "      <td>38.45N</td>\n",
       "      <td>39.16E</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>19:40:50</td>\n",
       "      <td>3.32S</td>\n",
       "      <td>135.40E</td>\n",
       "      <td>PAPUA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020/02/11</td>\n",
       "      <td>19:28:44</td>\n",
       "      <td>18.85N</td>\n",
       "      <td>155.21W</td>\n",
       "      <td>HAWAII REGION, HAWAII</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date      time     lat      lon                      region\n",
       "0   2020/02/11  21:36:02   0.14N  130.68E     PAPUA REGION, INDONESIA\n",
       "1   2020/02/11  21:31:13  17.85N   66.94W          PUERTO RICO REGION\n",
       "2   2020/02/11  21:21:19  16.34N   98.29W              OAXACA, MEXICO\n",
       "3   2020/02/11  21:10:51  27.90S   69.17W              ATACAMA, CHILE\n",
       "4   2020/02/11  21:04:21   4.74N   83.49E          NORTH INDIAN OCEAN\n",
       "5   2020/02/11  21:02:35  40.54N   20.88E                     ALBANIA\n",
       "6   2020/02/11  20:45:16  38.86N   17.43E              SOUTHERN ITALY\n",
       "7   2020/02/11  20:40:51  35.12N   27.91E  DODECANESE ISLANDS, GREECE\n",
       "8   2020/02/11  20:38:32   2.89S  129.85E            SERAM, INDONESIA\n",
       "9   2020/02/11  20:28:46  38.45N   25.67E                  AEGEAN SEA\n",
       "10  2020/02/11  20:23:50  17.89N   66.85W          PUERTO RICO REGION\n",
       "11  2020/02/11  20:15:44  38.84N  122.76W         NORTHERN CALIFORNIA\n",
       "12  2020/02/11  20:15:34  20.87S   70.82W    OFFSHORE TARAPACA, CHILE\n",
       "13  2020/02/11  20:15:32  19.17N  155.47W    ISLAND OF HAWAII, HAWAII\n",
       "14  2020/02/11  20:06:05  38.86N   16.81E              SOUTHERN ITALY\n",
       "15  2020/02/11  20:02:39  34.91N   27.94E   EASTERN MEDITERRANEAN SEA\n",
       "16  2020/02/11  19:55:58   2.95S  119.38E         SULAWESI, INDONESIA\n",
       "17  2020/02/11  19:52:36  38.45N   39.16E              EASTERN TURKEY\n",
       "18  2020/02/11  19:40:50   3.32S  135.40E            PAPUA, INDONESIA\n",
       "19  2020/02/11  19:28:44  18.85N  155.21W       HAWAII REGION, HAWAII"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "def clean_date(date):\n",
    "    date_c = re.search('\\d{4,}[-./|]\\d{2,}[-./|]\\d{2,}', date)\n",
    "    time_c = re.search('\\d{2,}:\\d{2,}:\\d{2,}', date)\n",
    "    if date:\n",
    "        str_date = date_c.group()\n",
    "        str_date = str_date[0:4] +'/'+ str_date[5:7] +'/'+ str_date[8:10]\n",
    "        str_time = time_c.group()\n",
    "    else: \n",
    "        str_date = None\n",
    "        str_time = None\n",
    "    return(pd.Series([str_date,str_time],index=['date','time']))\n",
    "\n",
    "\n",
    "earthquakes  = soup.find(id='tbody')\n",
    "earthquakes  = earthquakes.find_all(\"tr\")\n",
    "earthquakes = [e.find_all(\"td\") for e in earthquakes]\n",
    "list_earthquakes=[]\n",
    "for earth in earthquakes:\n",
    "    list_earthquakes.append([e.text.strip() for e in earth])\n",
    "\n",
    "colnames = ['D1','D2','D3','date_time','lat_1','lat_2','lon_1','lon_2',\n",
    "            'dep','mag_1','mag_2','region','last_update']\n",
    "df = pd.DataFrame(list_earthquakes, columns=colnames)\n",
    "df['lat']=df.lat_1+df.lat_2\n",
    "df['lon']=df.lon_1+df.lon_2 \n",
    "df[['date', 'time']] = df.date_time.apply(lambda x:clean_date(x))\n",
    "df_out=df[['date', 'time','lat','lon','region']]\n",
    "df_out.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count number of tweets by a given Twitter account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T19:58:56.279043Z",
     "start_time": "2020-02-11T19:58:56.273060Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T19:53:28.911734Z",
     "start_time": "2020-02-11T19:53:28.900436Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:42:14.362059Z",
     "start_time": "2020-02-11T22:42:14.354052Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to include a ***try/except block*** in case account/s name not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:39.083Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:39.090Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:58:43.889732Z",
     "start_time": "2020-02-11T21:58:43.550945Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:58:55.745578Z",
     "start_time": "2020-02-11T21:58:55.717393Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>5 994 000+ articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2 385 000+ Artikel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>2 171 000+ articles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Русский</td>\n",
       "      <td>1 590 000+ статей</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1 576 000+ voci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Español</td>\n",
       "      <td>1 571 000+ artículos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polski</td>\n",
       "      <td>1 379 000+ haseł</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日本語</td>\n",
       "      <td>1 185 000+ 記事</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中文</td>\n",
       "      <td>1 090 000+ 條目</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Português</td>\n",
       "      <td>1 018 000+ artigos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Language              Articles\n",
       "0    English   5 994 000+ articles\n",
       "2    Deutsch    2 385 000+ Artikel\n",
       "5   Français   2 171 000+ articles\n",
       "4    Русский     1 590 000+ статей\n",
       "6   Italiano       1 576 000+ voci\n",
       "3    Español  1 571 000+ artículos\n",
       "9     Polski      1 379 000+ haseł\n",
       "1        日本語         1 185 000+ 記事\n",
       "7         中文         1 090 000+ 條目\n",
       "8  Português    1 018 000+ artigos"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "list_twt = soup.find(\"div\", class_=\"central-featured\")\n",
    "list_twt = list_twt.find_all(\"a\")\n",
    "list_twt = [tweet.text.strip().split('\\n') for tweet in list_twt]\n",
    "\n",
    "colnames = ['Language','Articles']\n",
    "df_out = pd.DataFrame(list_twt, columns=colnames).sort_values(by=['Articles'], ascending=False)\n",
    "df_out.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:59:11.125541Z",
     "start_time": "2020-02-11T21:59:10.477309Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T21:59:12.397816Z",
     "start_time": "2020-02-11T21:59:12.387296Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business and economy', 'Crime and justice', 'Defence', 'Education', 'Environment', 'Government', 'Government spending', 'Health', 'Mapping', 'Society', 'Towns and cities', 'Transport']\n"
     ]
    }
   ],
   "source": [
    "#your code \n",
    "list_ds = soup.find(\"div\", class_=\"grid-row dgu-topics\")\n",
    "list_ds = list_ds.find_all(\"a\")\n",
    "list_ds = [ds.text for ds in list_ds]\n",
    "print(list_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:10:52.807283Z",
     "start_time": "2020-02-11T22:10:52.076665Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:15:01.882593Z",
     "start_time": "2020-02-11T22:15:01.840414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers</th>\n",
       "      <th>% of the World population</th>\n",
       "      <th>Language family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "      <td>11.922</td>\n",
       "      <td>Sino-TibetanSinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "      <td>5.994</td>\n",
       "      <td>Indo-EuropeanRomance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "      <td>4.922</td>\n",
       "      <td>Indo-EuropeanGermanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (Sanskritised Hindustani)[9]</td>\n",
       "      <td>341</td>\n",
       "      <td>4.429</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>228</td>\n",
       "      <td>2.961</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "      <td>2.870</td>\n",
       "      <td>Indo-EuropeanRomance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "      <td>2.000</td>\n",
       "      <td>Indo-EuropeanBalto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "      <td>1.662</td>\n",
       "      <td>JaponicJapanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Western Punjabi[10]</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Telugu</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1.065</td>\n",
       "      <td>DravidianSouth-Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Wu Chinese</td>\n",
       "      <td>81.4</td>\n",
       "      <td>1.057</td>\n",
       "      <td>Sino-TibetanSinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>79.4</td>\n",
       "      <td>1.031</td>\n",
       "      <td>TurkicOghuz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Korean</td>\n",
       "      <td>77.3</td>\n",
       "      <td>1.004</td>\n",
       "      <td>Koreaniclanguage isolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>French</td>\n",
       "      <td>77.2</td>\n",
       "      <td>1.003</td>\n",
       "      <td>Indo-EuropeanRomance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>German</td>\n",
       "      <td>76.1</td>\n",
       "      <td>0.988</td>\n",
       "      <td>Indo-EuropeanGermanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.987</td>\n",
       "      <td>AustroasiaticVietic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Tamil</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.974</td>\n",
       "      <td>DravidianSouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Yue Chinese</td>\n",
       "      <td>73.1</td>\n",
       "      <td>0.949</td>\n",
       "      <td>Sino-TibetanSinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Urdu (Persianised Hindustani)[9]</td>\n",
       "      <td>68.6</td>\n",
       "      <td>0.891</td>\n",
       "      <td>Indo-EuropeanIndo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                            Language Speakers  \\\n",
       "0     1                    Mandarin Chinese      918   \n",
       "1     2                             Spanish      480   \n",
       "2     3                             English      379   \n",
       "3     4  Hindi (Sanskritised Hindustani)[9]      341   \n",
       "4     5                             Bengali      228   \n",
       "5     6                          Portuguese      221   \n",
       "6     7                             Russian      154   \n",
       "7     8                            Japanese      128   \n",
       "8     9                 Western Punjabi[10]     92.7   \n",
       "9    10                             Marathi     83.1   \n",
       "10   11                              Telugu     82.0   \n",
       "11   12                          Wu Chinese     81.4   \n",
       "12   13                             Turkish     79.4   \n",
       "13   14                              Korean     77.3   \n",
       "14   15                              French     77.2   \n",
       "15   16                              German     76.1   \n",
       "16   17                          Vietnamese     76.0   \n",
       "17   18                               Tamil     75.0   \n",
       "18   19                         Yue Chinese     73.1   \n",
       "19   20    Urdu (Persianised Hindustani)[9]     68.6   \n",
       "\n",
       "   % of the World population            Language family  \n",
       "0                     11.922        Sino-TibetanSinitic  \n",
       "1                      5.994       Indo-EuropeanRomance  \n",
       "2                      4.922      Indo-EuropeanGermanic  \n",
       "3                      4.429    Indo-EuropeanIndo-Aryan  \n",
       "4                      2.961    Indo-EuropeanIndo-Aryan  \n",
       "5                      2.870       Indo-EuropeanRomance  \n",
       "6                      2.000  Indo-EuropeanBalto-Slavic  \n",
       "7                      1.662            JaponicJapanese  \n",
       "8                      1.204    Indo-EuropeanIndo-Aryan  \n",
       "9                      1.079    Indo-EuropeanIndo-Aryan  \n",
       "10                     1.065     DravidianSouth-Central  \n",
       "11                     1.057        Sino-TibetanSinitic  \n",
       "12                     1.031                TurkicOghuz  \n",
       "13                     1.004   Koreaniclanguage isolate  \n",
       "14                     1.003       Indo-EuropeanRomance  \n",
       "15                     0.988      Indo-EuropeanGermanic  \n",
       "16                     0.987        AustroasiaticVietic  \n",
       "17                     0.974             DravidianSouth  \n",
       "18                     0.949        Sino-TibetanSinitic  \n",
       "19                     0.891    Indo-EuropeanIndo-Aryan  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code\n",
    "list_ds = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "list_ds = list_ds.find_all(\"tr\")\n",
    "list_speakers =[]\n",
    "for speakers in list_ds:\n",
    "    speaker = speakers.find_all(\"td\")\n",
    "    list_speakers.append([x.text.strip().split(\"\\n\")[0] for x in speaker])\n",
    "# print(list_speakers)\n",
    "colnames = ['Rank','Language','Speakers','% of the World population','Language family']\n",
    "df_speakers = pd.DataFrame(list_speakers[1:], columns=colnames)\n",
    "df_speakers.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:40.286Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:40.295Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:19:11.577195Z",
     "start_time": "2020-02-11T22:19:10.242950Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "html=requests.get(url).content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T22:53:28.182836Z",
     "start_time": "2020-02-11T22:53:28.097334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['', '1.      Cadena perpetua(1994)', '9.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '2.      El padrino(1972)', '9.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '3.      El padrino: Parte II(1974)', '9.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '4.      El caballero oscuro(2008)', '9.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '5.      12 hombres sin piedad(1957)', '8.9', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '6.      La lista de Schindler(1993)', '8.9', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '7.      El señor de los anillos: El retorno del rey(2003)', '8.9', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '8.      Pulp Fiction(1994)', '8.9', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '9.      El bueno, el feo y el malo(1966)', '8.8', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '10.      El señor de los anillos: La comunidad del anillo(2001)', '8.8', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '11.      El club de la lucha(1999)', '8.8', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '12.      Forrest Gump(1994)', '8.8', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '13.      Origen(2010)', '8.7', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '14.      El imperio contraataca(1980)', '8.7', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '15.      El señor de los anillos: Las dos torres(2002)', '8.7', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '16.      Matrix(1999)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '17.      Uno de los nuestros(1990)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '18.      Alguien voló sobre el nido del cuco(1975)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '19.      Los siete samuráis(1954)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '20.      Seven(1995)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '21.      Parásitos(2019)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '22.      Ciudad de Dios(2002)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '23.      La vida es bella(1997)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '24.      El silencio de los corderos(1991)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '25.      ¡Qué bello es vivir!(1946)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '26.      La guerra de las galaxias(1977)', '8.6', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '27.      Salvar al soldado Ryan(1998)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '28.      El viaje de Chihiro(2001)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '29.      La milla verde(1999)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '30.      Interstellar(2014)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '31.      El profesional (Léon)(1994)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '32.      Sospechosos habituales(1995)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '33.      Harakiri(1962)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '34.      El rey león(1994)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '35.      American History X(1998)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '36.      Joker(2019)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '37.      Terminator 2: El juicio final(1991)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '38.      Regreso al futuro(1985)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '39.      El pianista de Roman Polanski(2002)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '40.      Tiempos modernos(1936)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '41.      Psicosis(1960)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '42.      Gladiator (El gladiador)(2000)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '43.      Luces de la ciudad(1931)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '44.      Intocable(2011)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '45.      Infiltrados(2006)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '46.      Whiplash(2014)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '47.      Hasta que llegó su hora(1968)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '48.      El truco final (El prestigio)(2006)', '8.5', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '49.      Casablanca(1942)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '50.      La tumba de las luciérnagas(1988)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '51.      La ventana indiscreta(1954)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '52.      Cinema Paradiso(1988)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '53.      Alien, el octavo pasajero(1979)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '54.      1917(2019)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '55.      En busca del arca perdida(1981)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '56.      Apocalypse Now(1979)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '57.      Memento(2000)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '58.      El gran dictador(1940)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '59.      La vida de los otros(2006)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '60.      Django desencadenado(2012)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '61.      Vengadores: Infinity War(2018)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '62.      Senderos de gloria(1957)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '63.      El resplandor(1980)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '64.      Vengadores: Endgame(2019)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '65.      WALL·E(2008)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '66.      Spider-Man: Un nuevo universo(2018)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '67.      La princesa Mononoke(1997)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '68.      El crepúsculo de los dioses(1950)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '69.      ¿Teléfono rojo? Volamos hacia Moscú(1964)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '70.      Old Boy(2003)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '71.      Testigo de cargo(1957)', '8.4', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '72.      El caballero oscuro: La leyenda renace(2012)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '73.      Érase una vez en América(1984)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '74.      Aliens: El regreso(1986)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '75.      Your Name.(2016)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '76.      American Beauty(1999)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '77.      Coco(2017)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '78.      Braveheart(1995)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '79.      El submarino - Das Boot(1981)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '80.      El infierno del odio(1963)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '81.      3 Idiots(2009)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '82.      Toy Story(1995)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '83.      El retorno del Jedi(1983)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '84.      Taare Zameen Par(2007)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '85.      Amadeus(1984)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '86.      Reservoir Dogs(1992)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '87.      Malditos bastardos(2009)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '88.      El indomable Will Hunting(1997)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '89.      2001: Una odisea del espacio(1968)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '90.      M, el vampiro de Düsseldorf(1931)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '91.      Réquiem por un sueño(2000)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '92.      Vértigo (De entre los muertos)(1958)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '93.      Dangal(2016)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '94.      ¡Olvídate de mí!(2004)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '95.      Ciudadano Kane(1941)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '96.      La caza(2012)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '97.      La chaqueta metálica(1987)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '98.      Con la muerte en los talones(1959)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '99.      La naranja mecánica(1971)', '8.3', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '100.      Snatch: Cerdos y diamantes(2000)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '101.      El chico(1921)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '102.      Ladrón de bicicletas(1948)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '103.      Cantando bajo la lluvia(1952)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '104.      El precio del poder(1983)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '105.      Taxi Driver(1976)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '106.      Amelie(2001)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '107.      Lawrence de Arabia(1962)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '108.      Toy Story 3(2010)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '109.      El golpe(1973)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '110.      Cafarnaúm(2018)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '111.      Metrópolis(1927)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '112.      La muerte tenía un precio(1965)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '113.      Vivir(1952)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '114.      Nader y Simin, una separación(2011)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '115.      Perdición(1944)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '116.      Matar a un ruiseñor(1962)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '117.      El apartamento(1960)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '118.      Indiana Jones y la última cruzada(1989)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '119.      Incendios(2010)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '120.      Up(2009)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '121.      L.A. Confidential(1997)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '122.      Los caballeros de la mesa cuadrada y sus locos seguidores(1975)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '123.      Heat(1995)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '124.      Jungla de cristal(1988)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '125.      Rashomon(1950)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '126.      Yojimbo(1961)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '127.      Batman Begins(2005)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '128.      Sin perdón(1992)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '129.      Green Book(2018)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '130.      El hundimiento(2004)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '131.      Children of Heaven(1997)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '132.      Con faldas y a lo loco(1959)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '133.      El castillo ambulante(2004)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '134.      Masacre (ven y mira)(1985)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '135.      La gran evasión(1963)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '136.      Ran(1985)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '137.      Mi vecino Totoro(1988)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '138.      Eva al desnudo(1950)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '139.      Una mente maravillosa(2001)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '140.      Casino(1995)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '141.      El laberinto del fauno(2006)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '142.      Toro salvaje(1980)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '143.      El secreto de sus ojos(2009)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '144.      Lock & Stock(1998)', '8.2', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '145.      El lobo de Wall Street(2013)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '146.      El tesoro de Sierra Madre(1948)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '147.      ¿Vencedores o vencidos?(1961)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '148.      Tres anuncios en las afueras(2017)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '149.      Mi padre y mi hijo(2005)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '150.      Chinatown(1974)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '151.      Pozos de ambición(2007)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '152.      La quimera del oro(1925)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '153.      Crimen perfecto(1954)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '154.      V de Vendetta(2005)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '155.      Del revés (Inside Out)(2015)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '156.      El séptimo sello(1957)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '157.      Warrior(2011)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '158.      No es país para viejos(2007)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '159.      Trainspotting(1996)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '160.      El hombre elefante(1980)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '161.      Shutter Island(2010)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '162.      La habitación(2015)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '163.      El sexto sentido(1999)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '164.      La cosa (El enigma de otro mundo)(1982)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '165.      Lo que el viento se llevó(1939)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '166.      Blade Runner(1982)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '167.      El puente sobre el río Kwai(1957)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '168.      Jurassic Park (Parque Jurásico)(1993)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '169.      El tercer hombre(1949)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '170.      La ley del silencio(1954)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '171.      Buscando a Nemo(2003)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '172.      Fresas salvajes(1957)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', \"173.      Le Mans '66(2019)\", '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '174.      Fargo(1996)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '175.      Gran Torino(2008)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '176.      Kill Bill: Volumen 1(2003)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '177.      El cazador(1978)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '178.      Andhadhun(2018)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '179.      El show de Truman (Una vida en directo)(1998)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '180.      Cuentos de Tokio(1953)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '181.      Stalker(1979)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '182.      Eskiya(1996)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '183.      Relatos salvajes(2014)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '184.      El gran Lebowski(1998)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '185.      Mary and Max(2009)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '186.      Memories of Murder (Crónica de un asesino en serie)(2003)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '187.      En el nombre del padre(1993)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '188.      Klaus(2019)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '189.      Perdida(2014)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '190.      Hasta el último hombre(2016)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '191.      Caballero sin espada(1939)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '192.      El gran hotel Budapest(2014)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '193.      Cómo entrenar a tu dragón(2010)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '194.      El maquinista de La General(1926)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '195.      El moderno Sherlock Holmes(1924)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '196.      Antes de amanecer(1995)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '197.      Persona(1966)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '198.      Atrápame si puedes(2002)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '199.      Prisioneros(2013)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '200.      12 años de esclavitud(2013)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '201.      La leyenda del indomable(1967)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '202.      Hacia rutas salvajes(2007)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '203.      El salario del miedo(1953)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '204.      Network, un mundo implacable(1976)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '205.      Cuenta conmigo(1986)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '206.      Mad Max: Furia en la carretera(2015)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '207.      La vida de Brian(1979)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '208.      Barry Lyndon(1975)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '209.      Platoon(1986)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '210.      Million Dollar Baby(2004)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '211.      Siempre a tu lado (Hachiko)(2009)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '212.      Rush(2013)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '213.      La pasión de Juana de Arco(1928)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '214.      Logan(2017)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '215.      Ben-Hur(1959)', '8.1', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '216.      Andrei Rublev(1966)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '217.      Rang De Basanti(2006)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '218.      Nausicaä del Valle del Viento(1984)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '219.      Harry Potter y las Reliquias de la Muerte - Parte 2(2011)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '220.      Los cuatrocientos golpes(1959)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '221.      El club de los poetas muertos(1989)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '222.      Amores perros(2000)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '223.      Hotel Rwanda(2004)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '224.      Spotlight(2015)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '225.      Rebeca(1940)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '226.      Rocky(1976)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '227.      La doncella(2016)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '228.      Monstruos, S.A.(2001)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '229.      Las zapatillas rojas(1948)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '230.      El gran carnaval(1951)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '231.      El odio(1995)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '232.      Sucedió una noche(1934)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '233.      Al rojo vivo(1949)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '234.      La princesa prometida(1987)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '235.      Lagaan: Érase una vez en la India(2001)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '236.      Antes del atardecer(2004)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '237.      Gangs of Wasseypur(2012)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '238.      Deseando amar(2000)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '239.      Criadas y señoras(2011)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '240.      Historia de un matrimonio(2019)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '241.      Terminator(1984)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '242.      Dos hombres y un destino(1969)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '243.      París, Texas(1984)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '244.      Akira(1988)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '245.      PK(2014)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '246.      Contratiempo(2016)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '247.      Aladdín(1992)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '248.      Guardianes de la galaxia(2014)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '249.      Mommy(2014)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n",
      "['', '250.      Winter Sleep (Sueño de invierno)(2014)', '8.0', '12345678910 NOT YET RELEASED Seen', '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "list_movies = soup.find(\"table\", class_=\"chart full-width\")\n",
    "list_movies = list_movies.find_all(\"tr\")\n",
    "list_top    =[]\n",
    "for movies in list_movies:\n",
    "    movie = movies.find_all(\"td\")\n",
    "    list_top.append([\"\".join(x.text.strip().split(\"\\n\")) for x in movie])\n",
    "[print(x) for x in list_top]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:40.812Z"
    }
   },
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:40.820Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:41.101Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = city=input('Enter the city:')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:41.107Z"
    }
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:41.355Z"
    }
   },
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-11T13:39:41.361Z"
    }
   },
   "outputs": [],
   "source": [
    "#your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
